{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Wiki Song Match Here you will find all the Documentation, Installation Guide, References and How to play the Game","title":"Welcome to Wiki Song Match"},{"location":"#welcome-to-wiki-song-match","text":"Here you will find all the Documentation, Installation Guide, References and How to play the Game","title":"Welcome to Wiki Song Match"},{"location":"Installation/","text":"Installation There are 2 \"sub-repos\" inside this repository that you can install: Full pipeline (Creating your own Vector Databse) Guessing game (Find a better song than there is in the database) Full Pipeline There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_full.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_full.txt 3) Download dataset from here. Save it as dataset.csv and put it inside the data folder. Otherwise, adapt the path inside load_embeddings_to_pineconde.py 4) Create an account in Pinecone and create an API key . 5) Create a .env file and add your API key inside the file. It should look like this: PINECONE_KEY=enterYourKeyHere 6) Load your data inside the vector database by running: python scripts/generate_lyrics_embeddings.py python scripts/load_embeddings_to_pinecone.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 7) Try the matching by running: python get_match.py -a <your-article> . If you don't pass an article, a random wikipedia article is taken. 8) Optional: Create your own TSNE plot: python scripts/generate_wikipedia_embeddings.py python scripts/create_tsne_plot.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 9) Optional: Host your own API To host your own API with flask, you also need an API key for Lyrics.com . Apply and wait for their email. In your .env append 2 more variables: LYRICS_USER=<YourUserID> LYRICS_TOKEN=<YourToken> 10) Optional: Deploy your own API on the cloud To deploy the flask API on the Google Cloud, follow this Tutorial Guessing Game If you already did the Full Pipeline installation, skip to step 3. There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_game.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_game.txt 3) Run the game with python game.py","title":"Installation"},{"location":"Installation/#installation","text":"There are 2 \"sub-repos\" inside this repository that you can install: Full pipeline (Creating your own Vector Databse) Guessing game (Find a better song than there is in the database)","title":"Installation"},{"location":"Installation/#full-pipeline","text":"There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_full.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_full.txt 3) Download dataset from here. Save it as dataset.csv and put it inside the data folder. Otherwise, adapt the path inside load_embeddings_to_pineconde.py 4) Create an account in Pinecone and create an API key . 5) Create a .env file and add your API key inside the file. It should look like this: PINECONE_KEY=enterYourKeyHere 6) Load your data inside the vector database by running: python scripts/generate_lyrics_embeddings.py python scripts/load_embeddings_to_pinecone.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 7) Try the matching by running: python get_match.py -a <your-article> . If you don't pass an article, a random wikipedia article is taken. 8) Optional: Create your own TSNE plot: python scripts/generate_wikipedia_embeddings.py python scripts/create_tsne_plot.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 9) Optional: Host your own API To host your own API with flask, you also need an API key for Lyrics.com . Apply and wait for their email. In your .env append 2 more variables: LYRICS_USER=<YourUserID> LYRICS_TOKEN=<YourToken> 10) Optional: Deploy your own API on the cloud To deploy the flask API on the Google Cloud, follow this Tutorial","title":"Full Pipeline"},{"location":"Installation/#guessing-game","text":"If you already did the Full Pipeline installation, skip to step 3. There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_game.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_game.txt 3) Run the game with python game.py","title":"Guessing Game"},{"location":"documentation/","text":"Research and Documentation Initial Ideas In the beginning, my main idea was to do something with embeddings. A spatial representation of data just seemed interesting to me, and I wanted to have a go at this myself and pack it into a creative project. I searched for data that is widely available and easy to process. Wikipedia articles were the first thing that came to my mind. I quickly searched the web and found a Python package to retrieve them easily. Now I needed something that I could compare the articles with. Nothing really came to my mind at first, but a few days later, I had the idea to match songs to Wikipedia articles. Getting the Embeddings Initial Approach My main goal for this project was to let the algorithm do all the matching itself, without interference from a human. So, I needed methods that required no labeling. Additionally, I wanted to use as many existing technologies as possible because the project seemed ambitious for a 3-credit module. With those goals in mind, the only really effective way to compare songs to articles was via the lyrics. To create a good matching algorithm, the embeddings needed to be as homogeneous as possible. This means the content of both texts should be as similar as possible before it gets converted to an embedding. I started off by trying to summarize the lyrics within 5 sentences using Gemini. For the Wikipedia articles, I already had a summary from the top of each page. This is what a prompt to the LLM looked like: \"Here are some lyrics from the song \" + title + \" by \" + artist + \". Write me a summary of the song. Dont try to interpret the lyrics, just summarize them. Dont speak about the lyrics being in a song and refer to the author as 'A person'. Take the lyrics and just assume that they are true. Do it with 5 sentences and only return the summarized text:\\n\" + lyrics, Now I plug the result into a pretrained Sentence-BERT, sum the embedding of each sentence, and normalize them. This is my final embedding of the 5 sentences. To check if the Wikipedia and lyrics embeddings are close by, I decided to use a dimensionality reduction method called TSNE so I could plot them in a 2D space. When I tested it with a random selection of songs and Wikipedia articles, the result was rather humbling: I played around a lot with the prompt and got similar results: Keywords to the Rescue Clearly, a new approach was needed. I didn't want to change the whole pipeline, though, because I had already spent a lot of time researching and building pipelines. My research led me to a Paper hat did \"Temporal Analysis and Visualization of Music.\" Inspired by this paper, I gave the keyword-based approach a go. Also, I used their dataset, which already has lemmatized lyrics of about 25,000 songs, so no more lyrics scraping on websites was required. Since I didn't want to copy their method entirely, I chose to go for another keyword extraction method and stumbled upon YAKE. Now I switched back to a regular BERT model and summed the embeddings of the top 10 keywords, normalizing them as well. This resulted in the following TSNE plot: While this doesn't look perfect, there definitely is more overlap between the embeddings. Without any other idea on how to improve my method, I decided to scale it to all 25,000 songs in my dataset. My intuition was that with a bigger sample size, we would have more individual songs that appear in the Wikipedia embeddings group. To visualize the final results, I added an interactive file called tsne_plot.html Matching The final matching is rather easy. We only need a good metric to tell us which lyrics embedding is closest to our desired Wikipedia embedding. The main contenders are Euclidean distance and cosine similarity. I chose to go for cosine similarity because it seems to be the standard. To do the matching, I actually use a vector database called Pinecone. Pinecone is not only used to store all my 25,000 lyrics embeddings, but you can also query for an embedding and get the top k closest to the specified embedding. This basically gets the job done for the matching part. Guessing Game Thanks to the questioning of Laura at my final presentation, I decided to create a super simple guessing game where you try to outmatch the \"AI\" and find a more suitable song for a random Wikipedia article. You can find more about that on the installation and game tab. Additional Info A lot of the things are not listed here in detail. But to cover them really quickly, I created a bullet point list for all the things that have also been done to make this project work: Dockerized code for cloud hosting. Created and hosted an API on the cloud for easier access to play the game. Created documentation using MkDocs that updates when code gets pushed. Added an installation guide to recreate the deployment from start to finish.","title":"Research and Documentation"},{"location":"documentation/#research-and-documentation","text":"","title":"Research and Documentation"},{"location":"documentation/#initial-ideas","text":"In the beginning, my main idea was to do something with embeddings. A spatial representation of data just seemed interesting to me, and I wanted to have a go at this myself and pack it into a creative project. I searched for data that is widely available and easy to process. Wikipedia articles were the first thing that came to my mind. I quickly searched the web and found a Python package to retrieve them easily. Now I needed something that I could compare the articles with. Nothing really came to my mind at first, but a few days later, I had the idea to match songs to Wikipedia articles.","title":"Initial Ideas"},{"location":"documentation/#getting-the-embeddings","text":"","title":"Getting the Embeddings"},{"location":"documentation/#initial-approach","text":"My main goal for this project was to let the algorithm do all the matching itself, without interference from a human. So, I needed methods that required no labeling. Additionally, I wanted to use as many existing technologies as possible because the project seemed ambitious for a 3-credit module. With those goals in mind, the only really effective way to compare songs to articles was via the lyrics. To create a good matching algorithm, the embeddings needed to be as homogeneous as possible. This means the content of both texts should be as similar as possible before it gets converted to an embedding. I started off by trying to summarize the lyrics within 5 sentences using Gemini. For the Wikipedia articles, I already had a summary from the top of each page. This is what a prompt to the LLM looked like: \"Here are some lyrics from the song \" + title + \" by \" + artist + \". Write me a summary of the song. Dont try to interpret the lyrics, just summarize them. Dont speak about the lyrics being in a song and refer to the author as 'A person'. Take the lyrics and just assume that they are true. Do it with 5 sentences and only return the summarized text:\\n\" + lyrics, Now I plug the result into a pretrained Sentence-BERT, sum the embedding of each sentence, and normalize them. This is my final embedding of the 5 sentences. To check if the Wikipedia and lyrics embeddings are close by, I decided to use a dimensionality reduction method called TSNE so I could plot them in a 2D space. When I tested it with a random selection of songs and Wikipedia articles, the result was rather humbling: I played around a lot with the prompt and got similar results:","title":"Initial Approach"},{"location":"documentation/#keywords-to-the-rescue","text":"Clearly, a new approach was needed. I didn't want to change the whole pipeline, though, because I had already spent a lot of time researching and building pipelines. My research led me to a Paper hat did \"Temporal Analysis and Visualization of Music.\" Inspired by this paper, I gave the keyword-based approach a go. Also, I used their dataset, which already has lemmatized lyrics of about 25,000 songs, so no more lyrics scraping on websites was required. Since I didn't want to copy their method entirely, I chose to go for another keyword extraction method and stumbled upon YAKE. Now I switched back to a regular BERT model and summed the embeddings of the top 10 keywords, normalizing them as well. This resulted in the following TSNE plot: While this doesn't look perfect, there definitely is more overlap between the embeddings. Without any other idea on how to improve my method, I decided to scale it to all 25,000 songs in my dataset. My intuition was that with a bigger sample size, we would have more individual songs that appear in the Wikipedia embeddings group. To visualize the final results, I added an interactive file called tsne_plot.html","title":"Keywords to the Rescue"},{"location":"documentation/#matching","text":"The final matching is rather easy. We only need a good metric to tell us which lyrics embedding is closest to our desired Wikipedia embedding. The main contenders are Euclidean distance and cosine similarity. I chose to go for cosine similarity because it seems to be the standard. To do the matching, I actually use a vector database called Pinecone. Pinecone is not only used to store all my 25,000 lyrics embeddings, but you can also query for an embedding and get the top k closest to the specified embedding. This basically gets the job done for the matching part.","title":"Matching"},{"location":"documentation/#guessing-game","text":"Thanks to the questioning of Laura at my final presentation, I decided to create a super simple guessing game where you try to outmatch the \"AI\" and find a more suitable song for a random Wikipedia article. You can find more about that on the installation and game tab.","title":"Guessing Game"},{"location":"documentation/#additional-info","text":"A lot of the things are not listed here in detail. But to cover them really quickly, I created a bullet point list for all the things that have also been done to make this project work: Dockerized code for cloud hosting. Created and hosted an API on the cloud for easier access to play the game. Created documentation using MkDocs that updates when code gets pushed. Added an installation guide to recreate the deployment from start to finish.","title":"Additional Info"},{"location":"game/","text":"Game Thanks to Laura's question after my final presentation, which gave me the idea for this game. How it Works After starting the game, you are presented with a random Wikipedia article and its summary. Your goal is to find a song that matches this Wikipedia article as closely as possible with its lyrics. Enter the song and artist name in each of the text fields. When you press \"Submit,\" your song gets searched and processed like every other song in the database. Meanwhile, a bot also selects a song from the 25,000 songs in the database. When the process is finished, you will be presented with a screen that shows the distance between the embeddings of your song choice and the Wikipedia article. The same goes for the bot. Whoever is closer wins! Disclaimer This game was coded in 1 day due to my limited time at the moment. You might experience crashes and unhandled exceptions, but those are to be expected. This game is mainly here to showcase the \"tech\" and interact with it in a fun way. It is limited to 100 requests a day due to the use of the free API tier for Lyrics.com. Also, I'm not a designer ;)","title":"Game"},{"location":"game/#game","text":"Thanks to Laura's question after my final presentation, which gave me the idea for this game.","title":"Game"},{"location":"game/#how-it-works","text":"After starting the game, you are presented with a random Wikipedia article and its summary. Your goal is to find a song that matches this Wikipedia article as closely as possible with its lyrics. Enter the song and artist name in each of the text fields. When you press \"Submit,\" your song gets searched and processed like every other song in the database. Meanwhile, a bot also selects a song from the 25,000 songs in the database. When the process is finished, you will be presented with a screen that shows the distance between the embeddings of your song choice and the Wikipedia article. The same goes for the bot. Whoever is closer wins!","title":"How it Works"},{"location":"game/#disclaimer","text":"This game was coded in 1 day due to my limited time at the moment. You might experience crashes and unhandled exceptions, but those are to be expected. This game is mainly here to showcase the \"tech\" and interact with it in a fun way. It is limited to 100 requests a day due to the use of the free API tier for Lyrics.com. Also, I'm not a designer ;)","title":"Disclaimer"},{"location":"references/","text":"References In here you find links used during research and implementation. Used in final product Lyrics Paper used for preprocessing steps and dataset Lyrics.com and their API Storage and Database Pinecone Pandas Preprocessing of Text Yake Keyword Extractor Spacy nltk Transformer Library (BERT) Game Pygame Background Image Deployment Google Cloud Deployment Used during research Google Gemini Spotify Python API ChatGPT Github Copilot Plotly in combination with TSNE","title":"References"},{"location":"references/#references","text":"In here you find links used during research and implementation.","title":"References"},{"location":"references/#used-in-final-product","text":"","title":"Used in final product"},{"location":"references/#lyrics","text":"Paper used for preprocessing steps and dataset Lyrics.com and their API","title":"Lyrics"},{"location":"references/#storage-and-database","text":"Pinecone Pandas","title":"Storage and Database"},{"location":"references/#preprocessing-of-text","text":"Yake Keyword Extractor Spacy nltk Transformer Library (BERT)","title":"Preprocessing of Text"},{"location":"references/#game","text":"Pygame Background Image","title":"Game"},{"location":"references/#deployment","text":"Google Cloud Deployment","title":"Deployment"},{"location":"references/#used-during-research","text":"Google Gemini Spotify Python API ChatGPT Github Copilot Plotly in combination with TSNE","title":"Used during research"}]}