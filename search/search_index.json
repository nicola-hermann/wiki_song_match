{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Installation/","text":"Installation There are 2 \"sub-repos\" inside this repository that you can install: Full pipeline (Creating your own Vector Databse) Guessing game (Find a better song than there is in the database) Full Pipeline There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_full.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_full.txt 3) Download dataset from here. Save it as dataset.csv and put it inside the data folder. Otherwise, adapt the path inside load_embeddings_to_pineconde.py 4) Create an account in Pinecone and create an API key . 5) Create a .env file and add your API key inside the file. It should look like this: PINECONE_KEY=enterYourKeyHere 6) Load your data inside the vector database by running: python scripts/generate_lyrics_embeddings.py python scripts/load_embeddings_to_pinecone.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 7) Try the matching by running: python get_match.py -a <your-article> . If you don't pass an article, a random wikipedia article is taken. 8) Optional: Create your own TSNE plot: python scripts/generate_wikipedia_embeddings.py python scripts/create_tsne_plot.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. Guessing Game If you already did the Full Pipeline installation, skip to step 3. There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_game.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_game.txt 3) Run the game with python guessing_game.py","title":"Installation"},{"location":"Installation/#installation","text":"There are 2 \"sub-repos\" inside this repository that you can install: Full pipeline (Creating your own Vector Databse) Guessing game (Find a better song than there is in the database)","title":"Installation"},{"location":"Installation/#full-pipeline","text":"There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_full.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_full.txt 3) Download dataset from here. Save it as dataset.csv and put it inside the data folder. Otherwise, adapt the path inside load_embeddings_to_pineconde.py 4) Create an account in Pinecone and create an API key . 5) Create a .env file and add your API key inside the file. It should look like this: PINECONE_KEY=enterYourKeyHere 6) Load your data inside the vector database by running: python scripts/generate_lyrics_embeddings.py python scripts/load_embeddings_to_pinecone.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps. 7) Try the matching by running: python get_match.py -a <your-article> . If you don't pass an article, a random wikipedia article is taken. 8) Optional: Create your own TSNE plot: python scripts/generate_wikipedia_embeddings.py python scripts/create_tsne_plot.py This process can take several hours! Note: the first script can be interruped and resumed any time. Progress is saved every 100 steps.","title":"Full Pipeline"},{"location":"Installation/#guessing-game","text":"If you already did the Full Pipeline installation, skip to step 3. There are alot of ways to install a repository. The following instructions are testen on Windows and MacOS. 1) Install python: This repository was built and tested on python 3.11.10 2) Create virtual environment and install requirements: Windows python -m venv .venv \".venv/Scripts/activate\" pip install -r requirements_game.txt MacOS python -m venv .venv source .venv/bin/activate pip insatll -r requirements_game.txt 3) Run the game with python guessing_game.py","title":"Guessing Game"},{"location":"references/","text":"References In here you find links used during research and implementation. Used in final product Lyrics Paper used for preprocessing steps and dataset Genius and their API Genius Python package Storage and Database Pinecone Pandas Preprocessing of Text Yake Keyword Extractor Spacy nltk Transformer Library (BERT) Deployment Google Cloud Deployment Used during research Google Gemini Spotify Python API ChatGPT Github Copilot Plotly in combination with TSNE https://upload.wikimedia.org/wikipedia/commons/c/cf/Artificial_Intelligence_Word_Cloud.png","title":"References"},{"location":"references/#references","text":"In here you find links used during research and implementation.","title":"References"},{"location":"references/#used-in-final-product","text":"","title":"Used in final product"},{"location":"references/#lyrics","text":"Paper used for preprocessing steps and dataset Genius and their API Genius Python package","title":"Lyrics"},{"location":"references/#storage-and-database","text":"Pinecone Pandas","title":"Storage and Database"},{"location":"references/#preprocessing-of-text","text":"Yake Keyword Extractor Spacy nltk Transformer Library (BERT)","title":"Preprocessing of Text"},{"location":"references/#deployment","text":"Google Cloud Deployment","title":"Deployment"},{"location":"references/#used-during-research","text":"Google Gemini Spotify Python API ChatGPT Github Copilot Plotly in combination with TSNE https://upload.wikimedia.org/wikipedia/commons/c/cf/Artificial_Intelligence_Word_Cloud.png","title":"Used during research"}]}